# 
global:
  model_name: "timeclr_first_approach"
  data_dir: "UCRArchive_2018/"
  max_len: 512  # Max length of time series (must match with out_dim)
  seed: 666

# Preprocess data params
preprocess:
  pretrain_frac: 0.65  # Por ahora se dividen por dataset, no por instancia.
  is_same_length: True

# Transformer architecture params
model:
  in_dim: 1           # Channels of TS
  out_dim: 512        # Number of dimensions for represent the data
  n_layer: 4          # Group of bocks of attention
  n_dim: 64           # Intern representation dimensions
  n_head: 8           # Attention heads
  norm_first: True    # If true. the norm will be applied before transformer sub-layers
  is_pos: True        # If positional encoding will be applied
  is_projector: True  # If projection will be applied
  project_norm: "LN"  # Id projection will be norm
  dropout: 0.0

#  Self-supervised learning task params
pretext_task:
  pretexttask_name: "timeclr"
  aug_bank_ver: 0

#  Training params
train:
  lr: 0.0001
  batch_size: 128
  n_epoch: 400
  # early_stopping: True # importante agregar en el futuro