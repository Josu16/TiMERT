# 
global:
  data_dir: "UCRArchive_2018/"
  max_len: 512  # Max length of time series (must match with out_dim)
  seed: 666

# Preprocess data params
preprocess:
  pretrain_frac: 0.65
  train_frac: 0.6
  valid_frac: 0.2
  test_frac: 0.2
  is_same_length: True

# Transformer architecture params
encoder:
  in_dim: 1           # Channels of TS
  out_dim: 512        # Number of dimensions for represent the data
  n_layer: 4          # Group of bocks of attention
  n_dim: 64           # Intern representation dimensions
  n_head: 8           # Attention heads
  norm_first: True    # If true. the norm will be applied before transformer sub-layers
  is_pos: True        # If positional encoding will be applied
  is_projector: True  # If projection will be applied
  project_norm: "LN"  # Id projection will be norm
  dropout: 0.0
  learnable_pos: False # If the positional encoder will be learnable (true), fixed (false)

#  Downstream task
classifier:
  n_dim: 64
  n_layer_class: 2

#  Training params
train:
  lr: 0.0001
  batch_size: 64
  n_epoch: 400 # con 10 va bien
  # early_stopping: True # importante agregar en el futuro